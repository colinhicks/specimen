<!doctype html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="../../css/theme.css">
    <link rel="stylesheet" href="../../css/github.css">
  </head>

  <body>
    <div>
    <h1>How ksqlDB works in 7 animations</h1>

    <p><a href="http://ksqldb.io/">ksqlDB</a>, the event streaming database, is becoming one of the most popular ways to work with Apache Kafka. Every day we answer many questions about the project, but here’s a question whose answer we always try to improve: how does it work?</p>

    <p>The mechanics behind stream processing can be challenging to grasp. The concepts are abstract, and many of them involve motion—two things that are hard for the mind’s eye to visualize. Let’s pop open the hood of ksqlDB, exploring its essential concepts, how they work, and how each relates to Kafka.</p>

    <p>If you like, you can follow along by executing the example code yourself. <a href="https://ksqldb.io/quickstart.html">ksqlDB's quickstart</a> makes it easy to get up and running.</p>

    <div>
      <h2>Declaring a stream</h2>
      <p>Stream processing is only interesting when you have more than one thing to process. In Kafka, you store a collection of events in a <em>topic</em>. Each event can contain any raw bytes that you want. In ksqlDB, you store events in a <em>stream</em>. A stream is a topic with a strongly defined schema. You declare it like this:</p>

      <pre class="narrative-code">
        <code class="lang-sql">
CREATE STREAM s2 (
    a VARCHAR KEY,
    b STRUCT<
        c VARCHAR,
        d INT
    >
) WITH (
    kafka_topic = 's2',
    partitions = 1,
    value_format = 'avro'
);
        </code>
      </pre>

      <p>When you fire off this statement, what actually happens? If the topic that backs this stream doesn’t exist, ksqlDB issues a call to the Kafka brokers to make a new topic with the specified number of partitions. The stream metadata, like the column layout, serialization scheme, and other information, is placed into ksqlDB’s <em>command topic</em>, which is its internal cluster communication channel. Each ksqlDB server materializes information from the command topic to a local metadata store, giving it a global catalog of objects.</p>

      <p>A newly declared stream has no data in it:</p>
      <div id="stream" style="width: 300px;">
      </div>
    </div>

    <div>
      <h2>Inserting rows</h2>
      <p>Empty collections aren’t that interesting either. In Kafka, you put a <em>record</em> into a topic. In ksqlDB, you put a <em>row</em> into a stream. A row is just a record with additional metadata. You <em>insert</em> rows like this:</p>

      <pre class="narrative-code"><code class="lang-sql">
INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    294, 'Die Hard::1988', 'action'
);

INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    354, 'Tree of Life::2011', 'drama'
);

INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    782, 'A Walk in the Clouds::1995', 'romance'
);

INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    128, 'The Big Lebowski::1998', 'comedy'
);
      </code></pre>

      <p>Each time you invoke an <code>INSERT</code> statement, a request with the payload is sent to a ksqlDB server. The server checks that the shape of the data is coherent with respect to the stream’s schema—malformed rows are rejected. If the row’s data types are sane, the server creates a record and automatically serializes its content using the scheme of choice as defined in the stream’s declaration. It uses the Kafka producer client to insert that record into the backing Kafka topic. All of the stream’s data lives directly on the broker. None of it lives in ksqlDB’s servers.</p>

      <p>After the inserts complete, the stream now looks like this. Hover over each row to see its contents. The data displayed describes the underlying Kafka record.</p>
      
      <div id="inserts" style="width: 300px;">
      </div>

      <p>Why does some of the row data end up in the key of the record and some in the value? ksqlDB superimposes a flat column abstraction on top of Kafka’s key/value model, but it doesn’t obliterate it.</p>

      <p>In the declaration of the stream, <code>col1</code> is qualified with the <code>KEY</code> keyword. That piece of syntax tells ksqlDB to store the data for that column in the key portion of the record—by default, columns are stored in the value. That is how the partition for each row is chosen. When ksqlDB produces the record to the underlying topic, its key content is hashed to select a partition for it to reside in. This causes all rows with the same key to be written to the same partition, which is a useful ordering guarantee.</p>
    </div>    

    <div>
      <h2>Transforming a stream</h2>
      <p>No one ever sends data to Kafka just to let it sit there. You always want to do something with it. And most often, the data isn’t yet in the exact form that you need to work with it. You need to change it in some way.</p>

      <p>The most elementary way you could do this is by writing a program that uses the Kafka producer and consumer clients. The program would read from the source topic whose data you want to change, apply a function to each message, and write a new record to the output topic. It would loop and run forever. This works, but it is rather low-level. You need to manage schemas, serializers, partitioning strategies, and other pieces of configuration.</p>

      <p>In ksqlDB, you issue a <em>persistent query</em> to <em>transform</em> one stream into another using its SQL programming model. You derive a new stream from an existing one by selecting and manipulating columns of interest:</p>

      <pre class="narrative-code"><code class="lang-sql">
CREATE STREAM s1_by_country AS
  SELECT buyer, amount, UCASE(country) AS country
  FROM s2
  EMIT CHANGES;
      </code></pre>

      <p>Persistent queries are little stream processing programs that run indefinitely. In this case, it continually reads rows from <code>s2</code>, applies the transformation logic, and writes rows to <code>s3</code>. You are relieved of all data-janitorial work: there are no schemas to manage, no serializers to configure, no partitioning strategies to choose. But what is actually happening when you launch this query?</p>

      <p>Each time you run a persistent query, ksqlDB server compiles the query's textual representation to a physical execution plan as a Kafka Streams topology. The topology runs as a daemon on the server, reacting to new topic records as soon as they become available. This means that all of the processing work happens on ksqlDB server: no processing work happens on the Kafka brokers. If you run ksqlDB as a cluster, the topology scales horizontally across the nodes by internally using Kafka Streams service IDs.</p>

      <p>When everything is connected together and the data is flowing, it looks like this:</p>

      <div id="transformation" style="width: 750px;">
      </div>

      <p>What is going on here? What do the moving arrows mean? Why are those numbers changing? And what is "<code>pq1</code>"?</p>

      <p>When a persistent query is created, it is assigned a generated name (in this case, we call it <code>pq1</code> for concision). Rows are read from the stream partitions that the query selects from. As each row passes through the persistent query, the transformation logic is applied to create a new row, which is what the change of color signifies.</p>

      <p>Persistent queries completely manage their own processing progression, even in the presence of faults. ksqlDB durably maintains the highest offset of each input partition. The incrementing numbers underneath the query box describe those values at each point in time. Moreover, the arrows that move from right to left on the input streams show the corresponding offsets, giving you a spatial sense of how far processing has progressed.</p>

      <p>Pause the animation and hover over the output rows. Notice how their contents are transformed, but the rest of their metadata is intact. ksqlDB has taken care of all the bookkeeping for you.</p>

      <p>As you watch the data flowing through the topology, you might be wondering how ksqlDB chooses which input partition it will read from next. Is it random? Is it round robin? The answer to that question is the foundation of how ksqlDB deals with out of order data, and it’s something that we’ll describe in a future blog post all on its own. (Spoiler: it picks the smallest timestamp available.)</p>
    </div>

    <div>
      <h2>Filtering rows out of a stream</h2>
      <p>Let’s look at another simple operation: filtering. Filters are used to discard rows that you do not need or want. Just like transforms, filters are specified using simple SQL syntax.</p>

      <pre class="narrative-code"><code class="lang-sql">
CREATE STREAM s4 AS
    SELECT *
      FROM s3
      WHERE author = 'George R. R. Martin';
      </code></pre>

      <p>When you write ksqlDB programs, you chain streams (and tables, which we’ll get to in the future) together. You create a figurative pathway for your data to traverse, with each step in the way performing a step of processing. ksqlDB handles the mechanics of how your data is propagated through the chain.</p>

      <div id="filtering" style="width: 750px;">
      </div>
    </div>

    <div>
      <h2>Combining operations into one</h2>
      <p>A crucial rule of thumb in data processing is that you should get rid of data that you don’t need as early as possible. The longer you keep irrelevant data around, the highest the cost to repeatedly store, process, and transfer it. If you use the Kafka client to process data, it is up to you to manage where each processing step takes place.</p>

      <p>In ksqlDB, you can combine a wide range of operations into a single query. It’s composable query syntax allows you to fuse discrete, yet logical conjoined, operations into one.</p>

      <pre class="narrative-code"><code class="lang-sql">
CREATE STREAM s1_by_country AS
  SELECT buyer, amount, UCASE(country) AS country
  FROM s2
  WHERE foo= ‘bar’
  EMIT CHANGES;
      </code></pre>
      
      <p>This persistent query supplants the previous two that we wrote. It has the advantage of performing all of the computation in one place, discarding rows as early as possible.</p>

      <div id="compressed" style="width: 750px;">
      </div>
    </div>

    <div>
      <h2>Rekeying a stream</h2>
      <p>No account of data processing is complete without a discussion of data locality. When you use a distributed system, data and computation are spread over a cluster of machines, each performing a small task that adds up to a larger operation. But even though your entire data corpus is available, you often want to perform processing over some smaller slice of it. Imagine that you’re building an analytics service. You might want to see what percentage of your users who live in Philadelphia are registered to vote. To do that, you need to gather only the records of users who live in the City of Brotherly Love. But where do you gather them when the data resides in different devices?</p>

      <p>In Kafka, partitioning controls data locality. Each partition lives in its entirety on a broker. That is why the choice of how you key your records is such a crucial one. If you use the Kafka clients to process your data, you need to be careful that you’ve set this up right. But in ksqlDB, this is just another SQL clause.</p>

      <pre class="narrative-code"><code class="lang-sql">
CREATE STREAM orders_by_country AS
  SELECT *
  FROM clean_orders
  PARTITION BY country
  EMIT CHANGES;
      </code></pre>

      <p>What happens when you execute this statement is that ksqlDB creates a new persistent query. It continually reads from <code>clean_orders</code>, applies any additional logic (none in this case, since it simply selects everything), and writes a new record to <code>orders_by_country</code> with a new key. The value of each key is the content of country. This has the effect of colocating all rows with the same country in the same partition. As we’ll explore in a future blog post, this colocation property is essential for stateful operations like streaming join and incremental aggregations.</p>

      <div id="rekeying" style="width: 750px;">
      </div>

      <p>Observe how all blocks of the same color end up on the same partition.</p>
    </div>

    <div>
      <h2>Processing with multiple consumers</h2>
      <p>Foo</p>

      <div id="multi-consumer" style="width: 750px;">
      </div>
    </div>

    <div>
      <h2>Learning more</h2>
      <p>Foo</p>
    </div>

    <script src="./bundle.js"></script>    
  </body>
</html>
