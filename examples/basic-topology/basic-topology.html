<!doctype html>

<html lang="en">
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="../../css/theme.css">
    <link rel="stylesheet" href="../../css/github.css">
  </head>

  <body>
    <h1>How ksqlDB works in 7 animations</h1>

    <p>ksqlDB, the event streaming database, is becoming one of the most popular ways to work with Apache Kafka. Every day we answer many questions about the project, but here’s a question whose answer we always try to improve: how does it work?</p>

    <p>The mechanics behind stream processing can be challenging to grasp. The concepts are abstract, and many of them involve motion—two things that are hard for the mind’s eye to visualize. In this blog, we shine a flightlight in the dark cave of ksqlDB. We illuminate its essential concepts, how they work, and how it all relates to Kafka.</p>

    <p>If you like, you can follow along by executing the example code in ksqlDB yourself. ksqlDB’s quickstart makes it easy to get up and running.</p>

    <div>
      <h2>Declaring a stream</h2>
      <p>Stream processing is only interesting when you have more than one thing to process. In Kafka, you store a collection of events in a <em>topic</em>. Each event can contain any raw bytes that you want. In ksqlDB, you store events in a <em>stream</em>. A stream is a topic with a strongly defined schema. You declare it like this:</p>

      <pre class="narrative-code">
        <code class="lang-sql">
CREATE STREAM s2 (
    a VARCHAR KEY,
    b STRUCT<
        c VARCHAR,
        d INT
    >
) WITH (
    kafka_topic = 's2',
    partitions = 1,
    value_format = 'avro'
);
        </code>
      </pre>

      <p>When you fire off this statement, what actually happens? If the topic that backs this stream doesn’t exist, ksqlDB issues a call to the Kafka brokers to make a new topic with the specified number of partitions. The stream metadata, like the column layout, serialization scheme, and other information, is placed into ksqlDB’s <em>command topic</em>, which is its internal cluster communication channel. Each ksqlDB server materializes information from the command topic to a local metadata store, giving it a global catalog of objects.</p>

      <p>A newly declared stream has no data in it:</p>
      <div id="stream" style="width: 300px;">
      </div>
    </div>

    <div>
      <h2>Inserting rows</h2>
      <p>Empty collections aren’t that interesting either. In Kafka, you put a <em>record</em> into a topic. In ksqlDB, you put a <em>row</em> into a stream. A row is just a record with additional metadata. You <em>insert</em> rows like this:</p>

      <pre class="narrative-code"><code class="lang-sql">
INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    294, 'Die Hard::1988', 'action'
);

INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    354, 'Tree of Life::2011', 'drama'
);

INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    782, 'A Walk in the Clouds::1995', 'romance'
);

INSERT INTO raw_movies (
    id, title, genre
) VALUES (
    128, 'The Big Lebowski::1998', 'comedy'
);
      </code></pre>

      <p>Each time you invoke an <code>INSERT</code> statement, a request with the payload is sent to a ksqlDB server. The server checks that the shape of the data is coherent with respect to the stream’s schema—malformed rows are rejected. If the row’s data types are sane, the server creates a record and automatically serializes its content using the scheme of choice as defined in the stream’s declaration. It uses the Kafka producer client to insert that record into the backing Kafka topic. All of the stream’s data lives directly on the broker. None of it lives in ksqlDB’s servers.</p>

      <p>After the inserts complete, the stream now looks like this. Hover over each row to see its contents. The data displayed describes the underlying Kafka record.</p>
      
      <div id="inserts" style="width: 300px;">
      </div>

      <p>Why does some of the row data end up in the key of the record and some in the value? ksqlDB superimposes a flat column abstraction on top of Kafka’s key/value model, but it doesn’t obliterate it. In the declaration of the stream, <code>col1</code> is qualified with the <code>KEY</code> keyword. That piece of syntax tells ksqlDB to store the data for that column in the key portion of the record—by default, columns are stored in the value. That is how the partition for each row is chosen. When ksqlDB produces the record to the underlying topic, its key content is hashed to select a partition for it to reside in. This causes all rows with the same key to be written to the same partition, which is a useful ordering guarantee.</p>
    </div>    

    <div>
      <h2>Transforming a stream</h2>
      <p>Foo</p>

      <div id="transformation" style="width: 750px;">
      </div>
    </div>

    <div>
      <h2>Filtering rows out of a stream</h2>
      <p>Foo</p>

      <div id="filtering" style="width: 750px;">
      </div>
    </div>

    <div>
      <h2>Combining operations into one</h2>
      <p>Foo</p>

      <div id="compressed" style="width: 750px;">
      </div>
    </div>

    <div>
      <h2>Rekeying a stream</h2>
      <p>Foo</p>

      <div id="rekeying" style="width: 750px;">
      </div>
    </div>

    <div>
      <h2>Processing with multiple consumers</h2>
      <p>Foo</p>

      <div id="multi-consumer" style="width: 750px;">
      </div>
    </div>
    
    <script src="./bundle.js"></script>    
  </body>
</html>
